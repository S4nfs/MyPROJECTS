/* 
    ____        _ __    ___                __________  _________ 
   / __ )__  __(_) /___/ (_)___  ____ _   / ____/ __ \/_  __/__ \
  / __  / / / / / / __  / / __ \/ __ `/  / / __/ /_/ / / /  __/ /
 / /_/ / /_/ / / / /_/ / / / / / /_/ /  / /_/ / ____/ / /  / __/ 
/_____/\__,_/_/_/\__,_/_/_/ /_/\__, /   \____/_/     /_/  /____/ 
                              /____/                             
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£∂‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£Ä‚£¥‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£∂‚£§‚£Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚°ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£Ä‚£¥‚£æ‚£ø‚†ü‚£¶‚£ç‚£à‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚£§‚£Ä‚£Ä‚£†‚£§‚£¥‚£∂‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ø‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ß‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚†ô‚†õ‚†õ‚†ø‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°è‚†Ä‚£†‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚£¥‚£ø‚£ø‚£ø‚£ø‚£ø‚£å‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ü‚†â‚†ô‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£§‚£Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚¢ã‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ü‚†Ä‚¢†‚£º‚£ø‚£æ‚£ø‚£ø‚£ø‚£ø‚†õ‚†õ‚†õ‚†õ‚†õ‚†Å‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£¥‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚†Å‚¢∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£§‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°Ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£¥‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚†Å‚†Ä‚†Ä‚¢∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°Ü‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†ê‚†õ‚†õ‚†õ‚†â‚†Å‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†õ‚¢â‚£Ä‚£Ä‚£§‚£§‚£§‚¢º‚£ø‚£ø‚£ü‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚†Ä‚†ô‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£á‚†Ä‚£Ä‚£ø‚£§‚£Ä‚†Ä‚†Ä‚†à‚£ø‚£ø‚£ø‚†à‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£Ñ‚£Ä‚†ô‚†ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†É‚†Ä‚†â‚†ô‚†õ‚†õ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚£¥‚°ø‚†ã‚†â‚£Ω‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ü‚†õ‚£ø‚£ø‚£∑‚£§‚†Ä‚†π‚£ø‚£ø‚£á‚†Ä‚†ô‚†ª‚£ø‚£ø‚£ø‚£ø‚£ß‚£ç‚£Ä‚†Ä‚†à‚†ô‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢π‚°∑‚¢Ñ‚£†‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚£ø‚£Ñ‚¢∏‚£ø‚£õ‚£ø‚°ó‚†Ä‚†π‚£ø‚£ø‚£Ñ‚†Ä‚¢∞‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∂‚£§‚£Ω‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚£ß‚†Ä‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á‚†ò‚†õ‚†õ‚†ø‚†ø‚†ø‚†á‚†Ä‚†Ä‚†Ä‚†à‚†ô‚†Ü‚†ò‚£ª‚£ø‚£∑‚£æ‚£ø‚£ø‚†â‚¢Ä‚£©‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚£ø‚¢ø‚£á‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†π‚£¶‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†â‚†õ‚†ã‚†â‚†â‚†ô‚†õ‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°∑‚£ø‚†§‚¢å‚°ô‚†≤‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°õ‚†õ‚†õ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°í‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚£Ä‚£¥‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚¢Å‚°ü‚†Ä‚†Ä‚†à‚†Ä‚†Ä‚†ô‚†≥‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚°Ä‚†Ä‚†Ä‚†Ä‚†ò‚¢¶‚£Ä‚£Ä‚£§‚£§‚£§‚£§‚£¥‚£ø‚†Å‚†Ä‚†Ä‚†Ä‚†õ‚†ø‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†É‚°º‚†Å‚†Ä‚¢Ä‚£Ä‚£Ä‚°§‚†§‚†§‚†§‚†ø‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£ø‚°ø‚†õ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚°Ä‚†Ä‚†Ä‚†Ä‚†ô‚¢ß‚°û‚†ã‚†Ä‚†Ä‚†Å‚°è‚††‚£§‚£§‚£§‚£§‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∂‚£ø‚£ø‚£ø‚£ø‚£ß‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚°ø‚†ã‚†Å‚¢Ä‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚£Ä‚†Ä‚†Ä‚†Ä‚†ô‚†¢‚¢§‚£†‚†û‚†Å‚†Ä‚†Ä‚†â‚†õ‚†ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ü‚†ø‚£ø‚£¶‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†õ‚†Å‚†Ä‚†Ä‚†Ä‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£¶‚£Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ä‚£§‚£¥‚†ñ‚†ã‚£Ω‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á‚†Ä‚†Ä‚†â‚†â‚†É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£æ‚†ü‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£æ‚£ø‚£ø‚£ø‚†ø‚†ã‚†Å‚¢†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£á‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚†â‚†Ä‚£∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†â‚†ô‚†õ‚†õ‚†ª‚†ø‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ä‚£Ä‚£§‚£æ‚£ø‚£ø‚†ø‚†õ‚†ã‚†ô‚¢ø‚£ø‚°ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ü‚†â‚†ô‚¢∑‚£§‚£Ä‚£Ä‚£Ä‚£Ä‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚†â‚¢ô‚£ø‚†ü‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†õ‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚¢∑‚£Ñ‚†Ä‚†Ä‚†Ä‚†â‚†â‚¢π‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†õ‚†õ‚†õ‚†ø‚†ø‚†ø‚£ø‚°ø‚£ø‚£ø‚†ø‚£ø‚£∑‚£§‚°§‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£¥‚†ü‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚†Ä‚†Ä‚£º‚£ø‚°ü‚†ª‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†â‚†É‚†Ä‚†Ä‚†Ä‚†ö‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†Ä‚†Ä‚†Ä‚†ô‚¢ß‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚°æ‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£∞‚°ü‚†Å‚†Ä‚†Ä‚£ø‚£ø‚†Å‚†Ä‚†à‚†ª‚£ø‚£ø‚£ø‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£ø‚£ø‚£ø‚†ø‚†ø‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚¢ª‚°Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚°º‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£º‚°ø‚†É‚†Ä‚¢Ä‚£º‚°ü‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†ª‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢∏‚£ø‚°ø‚†É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚†é‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢ª‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚£†‚°ü‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£æ‚†ü‚†Ä‚†Ä‚†Ä‚†ö‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†π‚£ø‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚†∏‚£ø‚†É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚£∑‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚¢Ä‚£¥‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚°æ‚£ã‚£Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢π‚£∑‚†Ä‚†Ä‚†Ä‚†Ä‚¢∏‚°ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢∏‚°Ü‚†Ä‚†Ä‚†Ä‚†Ä
‚£§‚†û‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚†ø‚†ø‚¢ø‚°Ü‚†Ä‚¢∞‚£æ‚†Ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ª‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢ø‚†Ä‚†Ä‚†Ä‚†Ä
‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚†è‚†Ä‚¢ø‚£ø‚£∂‚£§‚£Ñ‚†Ä‚¢Ä‚£ø‚°ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†Ä‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†∏‚°á‚†Ä‚†Ä‚†Ä
‚†ô‚†£‚£Ä‚°Ä‚£Ä‚°Ä‚£†‚£ø‚°É‚£¥‚°Ä‚†Ä‚†Ä‚†â‚¢π‚£ø‚°á‚£∏‚£ø‚£µ‚£æ‚£ø‚£ø‚£∂‚£Ä‚£§‚£∂‚£∂‚£§‚£Ä‚£†‚£¥‚£∂‚£¶‚£Ñ‚†Ä‚£†‚°Ñ‚†Ä‚£§‚°Ñ‚†Ä‚†à‚†≥‚£ø‚°Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£Ä‚£§‚£∑‚£¶‚£§‚†§
The project is based on building Generatively Pretrained Transformer (GPT) - a deep neural network model based on the Transformer architecture, with respect to "Attention is All You Need" and OpenAI's GPT-2 / GPT-3 model approach by Andrej Karpathy (co-founder of OpenAI). 

The model will be trained on one of the Shakespeare's famous play and my favourite "As You Like It" for the dataset.

References: 
https://jalammar.github.io/illustrated-gpt2/
https://arxiv.org/abs/1706.03762
*/

import axios from 'axios'
import * as cheerio from 'cheerio'
import fs from 'fs'
import path from 'path'
import { fileURLToPath } from 'url'
import * as tf from '@tensorflow/tfjs'

//üçúStart with a dataset to train on. Let's download the shakespeare play for the dataset-------------------------
async function fetchWebsiteText(url) {
  try {
    // Fetch the HTML of the website
    const { data } = await axios.get(url)

    // Load the HTML into Cheerio
    const $ = cheerio.load(data)

    // Get all text content from the body and saving in txt
    const textContent = $('body').text()
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const filePath = path.join(__dirname, 'As-You-Like-It.txt')
    await fs.writeFile(filePath, textContent, (err) => {
      if (err) {
        console.error(`Error writing to file: ${err}`)
      } else {
        console.log('File written successfully')
      }
    })
  } catch (error) {
    console.error(`Error fetching the webpage: ${error}`)
  }
}

// Download the whole play "As You Like It - By William Shakespeare" in a file (Already done)
// fetchWebsiteText('https://shakespeare.mit.edu/asyoulikeit/full.html')

async function GPT() {
  try {
    //ü¶¥ All the unique characters that occur in this text-----------------------------------------------------------

    const text = await fs.promises.readFile('As-You-Like-It.txt', 'utf8')
    // Convert the text into a set of unique characters and then sort them
    const chars = Array.from(new Set(text)).sort()

    // Calculate vocabulary size
    const vocabSize = chars.length

    // Output the sorted characters and vocabulary size
    console.log(chars.join('')) // Output: !&',-.:;?ABCDEFGHIJKLMNOPQRSTUVWXY[]abcdefghijklmnopqrstuvwxyz|
    console.log(vocabSize) // Output:66

    // Create mappings from characters to integers (stoi) and integers to characters (itos)
    const stoi = {}
    const itos = {}
    chars.forEach((ch, i) => {
      stoi[ch] = i
      itos[i] = ch
    })

    // Encoder function: takes a string, outputs a list of integers
    const encode = (s) => s.split('').map((c) => stoi[c])

    // Decoder function: takes a list of integers, outputs a string
    const decode = (l) => l.map((i) => itos[i]).join('')

    // Example usage
    const encoded = encode('hii there')
    console.log(encoded) // Output: [46, 47, 47,  2, 58, 46, 43, 56, 43]

    const decoded = decode(encoded)
    console.log(decoded) // Output: "hii there"

    const encodedText = encode(text)
    const data = tf.tensor(encodedText, [encodedText.length], 'int32') // 'int32' is similar to torch.long in PyTorch

    // Print the shape and dtype
    console.log('Shape:', data.shape) // [ 122983 ]
    console.log('Dtype:', data.dtype)

    // Display the first 1000 encoded characters
    const firstThousand = data.slice([0], [1000])
    firstThousand.print()

    //üçµ Example of Creating single batche of chunks data---------------------------------------------------------------------------------------------
    // Split the data into training and validation sets 90% will be used for training, the rest 10% for validation
    // const n = Math.floor(0.9 * data.shape[0]) // 90% of data for training
    // const trainData = data.slice([0], [n]) // Slicing from 0 to n for training data
    // const valData = data.slice([n], [data.shape[0] - n]) // Slicing from n to the end for validation data

    // // Define the block size or context length
    let blockSize = 8
    // const trainDataSubset = trainData.slice([0], [blockSize + 1]) // 9 characters from training set

    // const x = trainData.slice([0], [blockSize]) // The first 'blockSize' elements of the training data
    // const y = trainData.slice([1], [blockSize + 1]) // The next 'blockSize' elements, starting from index 1

    // for (let t = 0; t < blockSize; t++) {
    //   const context = x.slice([0], [t + 1]) // The input context, from the start to the current index
    //   const target = y.slice([t], [1]) // The target is the current element in 'y'
    //   console.log(`when input is ${context.dataSync()} the target: ${target.dataSync()}`)
    // }

    //üë©‚Äçüç≥ Creating batches of X of tensors and Y of targets to feed parallel to transformer---------------------------------------------------
    // Set manual seed for reproducibility
    function setSeed(seed) {
      //Function to create random number generator on a machine that spits out numbers in a seemingly random way. However, if you tell the machine to always start from the same "starting point," it will spit out the same sequence of "random" numbers every time you use it. This starting point is what we call a seed.
      tf.randomUniform([seed])
    }
    setSeed(1337)

    const batchSize = 4 // Number of independent sequences to process in parallel
    blockSize = 8 // Maximum context length for predictions

    // Simulate some train and validation data for demonstration
    const trainData = tf.range(0, 100, 1).arraySync() // Generate dummy train data as array
    const valData = tf.range(100, 200, 1).arraySync() // Generate dummy validation data as array

    // Function to get a batch of data
    function getBatch(split) {
      const data = split === 'train' ? trainData : valData
      const ix = Array.from({ length: batchSize }, () => Math.floor(Math.random() * (data.length - blockSize)))

      const x = ix.map((i) => data.slice(i, i + blockSize))
      const y = ix.map((i) => data.slice(i + 1, i + blockSize + 1))

      return [tf.tensor2d(x), tf.tensor2d(y)]
    }

    // Generate a training batch
    const [xb, yb] = getBatch('train')
    console.log('inputs:')
    xb.print() // TensorFlow.js method to print tensor values
    console.log('targets:')
    yb.print()

    console.log('----')

    // Iterate over batch and time dimensions to display context and target
    for (let b = 0; b < batchSize; b++) {
      // Batch dimension
      for (let t = 0; t < blockSize; t++) {
        // Time dimension
        const context = xb.slice([b, 0], [1, t + 1]).arraySync()[0]
        const target = yb.arraySync()[b][t]
        console.log(`when input is ${context} the target: ${target}`)
      }
    }

    // Clean up tensors to avoid memory leaks
    xb.dispose()
    yb.dispose()
  } catch (err) {
    console.error(`GPT Error: ${err}`)
  }
}

GPT()
